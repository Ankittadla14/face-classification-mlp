{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d9d9b8-7551-4bc8-8d82-65e5dc3c2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.707534  [    0/21100]\n",
      "loss: 0.457821  [12800/21100]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.341926  [    0/21100]\n",
      "loss: 0.357534  [12800/21100]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.302955  [    0/21100]\n",
      "loss: 0.327587  [12800/21100]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.309061  [    0/21100]\n",
      "loss: 0.280995  [12800/21100]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.309626  [    0/21100]\n",
      "loss: 0.343279  [12800/21100]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.335524  [    0/21100]\n",
      "loss: 0.363918  [12800/21100]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.348903  [    0/21100]\n",
      "loss: 0.230895  [12800/21100]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.307178  [    0/21100]\n",
      "loss: 0.291224  [12800/21100]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.242720  [    0/21100]\n",
      "loss: 0.396511  [12800/21100]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.298447  [    0/21100]\n",
      "loss: 0.276276  [12800/21100]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.279020  [    0/21100]\n",
      "loss: 0.307629  [12800/21100]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.280162  [    0/21100]\n",
      "loss: 0.284998  [12800/21100]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.219334  [    0/21100]\n",
      "loss: 0.322568  [12800/21100]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.241115  [    0/21100]\n",
      "loss: 0.251455  [12800/21100]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.234003  [    0/21100]\n",
      "loss: 0.277845  [12800/21100]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.291369  [    0/21100]\n",
      "loss: 0.260936  [12800/21100]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.409573  [    0/21100]\n",
      "loss: 0.298046  [12800/21100]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.260234  [    0/21100]\n",
      "loss: 0.310231  [12800/21100]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.216325  [    0/21100]\n",
      "loss: 0.259261  [12800/21100]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.273760  [    0/21100]\n",
      "loss: 0.221150  [12800/21100]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.197598  [    0/21100]\n",
      "loss: 0.248757  [12800/21100]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.241254  [    0/21100]\n",
      "loss: 0.285672  [12800/21100]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.252849  [    0/21100]\n",
      "loss: 0.276108  [12800/21100]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.275307  [    0/21100]\n",
      "loss: 0.234312  [12800/21100]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.190281  [    0/21100]\n",
      "loss: 0.225230  [12800/21100]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.248963  [    0/21100]\n",
      "loss: 0.258719  [12800/21100]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.237544  [    0/21100]\n",
      "loss: 0.212581  [12800/21100]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.227445  [    0/21100]\n",
      "loss: 0.216828  [12800/21100]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.194708  [    0/21100]\n",
      "loss: 0.188755  [12800/21100]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.184798  [    0/21100]\n",
      "loss: 0.221137  [12800/21100]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.186267  [    0/21100]\n",
      "loss: 0.276187  [12800/21100]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.209402  [    0/21100]\n",
      "loss: 0.211513  [12800/21100]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.206960  [    0/21100]\n",
      "loss: 0.201371  [12800/21100]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.195408  [    0/21100]\n",
      "loss: 0.220324  [12800/21100]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.197465  [    0/21100]\n",
      "loss: 0.196655  [12800/21100]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.229489  [    0/21100]\n",
      "loss: 0.246191  [12800/21100]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.201723  [    0/21100]\n",
      "loss: 0.176872  [12800/21100]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.209919  [    0/21100]\n",
      "loss: 0.183422  [12800/21100]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.189891  [    0/21100]\n",
      "loss: 0.186324  [12800/21100]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.163397  [    0/21100]\n",
      "loss: 0.220393  [12800/21100]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.216096  [    0/21100]\n",
      "loss: 0.195032  [12800/21100]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.172517  [    0/21100]\n",
      "loss: 0.192678  [12800/21100]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.190485  [    0/21100]\n",
      "loss: 0.158572  [12800/21100]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.230705  [    0/21100]\n",
      "loss: 0.208697  [12800/21100]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.156133  [    0/21100]\n",
      "loss: 0.250903  [12800/21100]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.177067  [    0/21100]\n",
      "loss: 0.154573  [12800/21100]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.189036  [    0/21100]\n",
      "loss: 0.231780  [12800/21100]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.184892  [    0/21100]\n",
      "loss: 0.158619  [12800/21100]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.202835  [    0/21100]\n",
      "loss: 0.169793  [12800/21100]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.177246  [    0/21100]\n",
      "loss: 0.169948  [12800/21100]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.223050  [    0/21100]\n",
      "loss: 0.170579  [12800/21100]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.176137  [    0/21100]\n",
      "loss: 0.162505  [12800/21100]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.185861  [    0/21100]\n",
      "loss: 0.226201  [12800/21100]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.163294  [    0/21100]\n",
      "loss: 0.182674  [12800/21100]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.178954  [    0/21100]\n",
      "loss: 0.186640  [12800/21100]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.177504  [    0/21100]\n",
      "loss: 0.186545  [12800/21100]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.195698  [    0/21100]\n",
      "loss: 0.153126  [12800/21100]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.137887  [    0/21100]\n",
      "loss: 0.136927  [12800/21100]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.150817  [    0/21100]\n",
      "loss: 0.136187  [12800/21100]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.152910  [    0/21100]\n",
      "loss: 0.189896  [12800/21100]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.184047  [    0/21100]\n",
      "loss: 0.170435  [12800/21100]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.173092  [    0/21100]\n",
      "loss: 0.154062  [12800/21100]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.155152  [    0/21100]\n",
      "loss: 0.139748  [12800/21100]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.151166  [    0/21100]\n",
      "loss: 0.166652  [12800/21100]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.147346  [    0/21100]\n",
      "loss: 0.153524  [12800/21100]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.154221  [    0/21100]\n",
      "loss: 0.248827  [12800/21100]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.177868  [    0/21100]\n",
      "loss: 0.176879  [12800/21100]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.166536  [    0/21100]\n",
      "loss: 0.207468  [12800/21100]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.182544  [    0/21100]\n",
      "loss: 0.161144  [12800/21100]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.182001  [    0/21100]\n",
      "loss: 0.169398  [12800/21100]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.203152  [    0/21100]\n",
      "loss: 0.153384  [12800/21100]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.163025  [    0/21100]\n",
      "loss: 0.134744  [12800/21100]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.137690  [    0/21100]\n",
      "loss: 0.181069  [12800/21100]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.137186  [    0/21100]\n",
      "loss: 0.164424  [12800/21100]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.161095  [    0/21100]\n",
      "loss: 0.161432  [12800/21100]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.191277  [    0/21100]\n",
      "loss: 0.131622  [12800/21100]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.149064  [    0/21100]\n",
      "loss: 0.157189  [12800/21100]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.172785  [    0/21100]\n",
      "loss: 0.182591  [12800/21100]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.155109  [    0/21100]\n",
      "loss: 0.154466  [12800/21100]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.144686  [    0/21100]\n",
      "loss: 0.162614  [12800/21100]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.173435  [    0/21100]\n",
      "loss: 0.155101  [12800/21100]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.168635  [    0/21100]\n",
      "loss: 0.226709  [12800/21100]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.153364  [    0/21100]\n",
      "loss: 0.142207  [12800/21100]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.196777  [    0/21100]\n",
      "loss: 0.191255  [12800/21100]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.152782  [    0/21100]\n",
      "loss: 0.150667  [12800/21100]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.173789  [    0/21100]\n",
      "loss: 0.144191  [12800/21100]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.167065  [    0/21100]\n",
      "loss: 0.134519  [12800/21100]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.159730  [    0/21100]\n",
      "loss: 0.177315  [12800/21100]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.183823  [    0/21100]\n",
      "loss: 0.136962  [12800/21100]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.166040  [    0/21100]\n",
      "loss: 0.133331  [12800/21100]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.143643  [    0/21100]\n",
      "loss: 0.136763  [12800/21100]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.128493  [    0/21100]\n",
      "loss: 0.162599  [12800/21100]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.169478  [    0/21100]\n",
      "loss: 0.137904  [12800/21100]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.140892  [    0/21100]\n",
      "loss: 0.131913  [12800/21100]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.173575  [    0/21100]\n",
      "loss: 0.141231  [12800/21100]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.160402  [    0/21100]\n",
      "loss: 0.150888  [12800/21100]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.136482  [    0/21100]\n",
      "loss: 0.160772  [12800/21100]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.239712  [    0/21100]\n",
      "loss: 0.174889  [12800/21100]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.128709  [    0/21100]\n",
      "loss: 0.168237  [12800/21100]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.213305  [    0/21100]\n",
      "loss: 0.156242  [12800/21100]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.139737  [    0/21100]\n",
      "loss: 0.146291  [12800/21100]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.168804  [    0/21100]\n",
      "loss: 0.233490  [12800/21100]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.131065  [    0/21100]\n",
      "loss: 0.177013  [12800/21100]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.135043  [    0/21100]\n",
      "loss: 0.126769  [12800/21100]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.133184  [    0/21100]\n",
      "loss: 0.132420  [12800/21100]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.133667  [    0/21100]\n",
      "loss: 0.160328  [12800/21100]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.147497  [    0/21100]\n",
      "loss: 0.169477  [12800/21100]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.134086  [    0/21100]\n",
      "loss: 0.131636  [12800/21100]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.160795  [    0/21100]\n",
      "loss: 0.154582  [12800/21100]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.143687  [    0/21100]\n",
      "loss: 0.150476  [12800/21100]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.131008  [    0/21100]\n",
      "loss: 0.136409  [12800/21100]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.130961  [    0/21100]\n",
      "loss: 0.135625  [12800/21100]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.148424  [    0/21100]\n",
      "loss: 0.124952  [12800/21100]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.147852  [    0/21100]\n",
      "loss: 0.129143  [12800/21100]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.141986  [    0/21100]\n",
      "loss: 0.128880  [12800/21100]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.123264  [    0/21100]\n",
      "loss: 0.125099  [12800/21100]\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.128419  [    0/21100]\n",
      "loss: 0.123578  [12800/21100]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.133426  [    0/21100]\n",
      "loss: 0.146023  [12800/21100]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.124099  [    0/21100]\n",
      "loss: 0.136524  [12800/21100]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.130996  [    0/21100]\n",
      "loss: 0.145702  [12800/21100]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.151429  [    0/21100]\n",
      "loss: 0.150669  [12800/21100]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.159700  [    0/21100]\n",
      "loss: 0.167527  [12800/21100]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.141623  [    0/21100]\n",
      "loss: 0.133029  [12800/21100]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.141212  [    0/21100]\n",
      "loss: 0.126184  [12800/21100]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.147935  [    0/21100]\n",
      "loss: 0.140437  [12800/21100]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.122179  [    0/21100]\n",
      "loss: 0.140780  [12800/21100]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.164707  [    0/21100]\n",
      "loss: 0.135999  [12800/21100]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.167968  [    0/21100]\n",
      "loss: 0.147377  [12800/21100]\n",
      "Optimization Finished!\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.361267 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Comparing single layer MLP with deep MLP (using PyTorch)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Create model\n",
    "\n",
    "def create_multilayer_perceptron():\n",
    "\n",
    "    class net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            n_input   = 2376\n",
    "            n_hidden1 = 512\n",
    "            n_hidden2 = 256\n",
    "            n_hidden3 = 64\n",
    "            n_classes = 2\n",
    "\n",
    "            # layer 1\n",
    "            self.fc1 = nn.Linear(n_input, n_hidden1)\n",
    "            self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "            self.do1 = nn.Dropout(p=0.30)\n",
    "\n",
    "            # layer 2\n",
    "            self.fc2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "            self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "            self.do2 = nn.Dropout(p=0.40)\n",
    "\n",
    "            # layer 3 (no BN/Dropout here is fine)\n",
    "            self.fc3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "\n",
    "            # output\n",
    "            self.out = nn.Linear(n_hidden3, n_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.do1(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.do2(x)\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.out(x)\n",
    "            return x\n",
    "\n",
    "    return net()\n",
    "\n",
    "\n",
    "# Do not change this\n",
    "def preprocess():\n",
    "    pickle_obj = pickle.load(file=open(\"C:/Users/ankit/OneDrive/Desktop/my_projects/face recognition/face_all.pickle\", 'rb'))\n",
    "    features = pickle_obj['Features']\n",
    "    labels = pickle_obj['Labels']\n",
    "    train_x = features[0:21100] / 255\n",
    "    valid_x = features[21100:23765] / 255\n",
    "    test_x = features[23765:] / 255\n",
    "\n",
    "    labels = np.squeeze(labels)\n",
    "    train_y = labels[0:21100]\n",
    "    valid_y = labels[21100:23765]\n",
    "    test_y = labels[23765:]\n",
    "\n",
    "    class dataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    trainset = dataset(train_x, train_y)\n",
    "    validset = dataset(valid_x, valid_y)\n",
    "    testset = dataset(test_x, test_y)\n",
    "\n",
    "    return trainset, validset, testset\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0003\n",
    "training_epochs = 128\n",
    "batch_size = 128\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Construct model\n",
    "model = create_multilayer_perceptron().to(device)\n",
    "\n",
    "# Define loss and openptimizer\n",
    "cost = nn.CrossEntropyLoss(label_smoothing=0.05)  # helps generalization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# load data\n",
    "trainset, validset, testset = preprocess()\n",
    "train_dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Training cycle\n",
    "for t in range(training_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "print(\"Optimization Finished!\")\n",
    "test(test_dataloader, model, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c669e52-c354-4a59-897d-08cc66eb824a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
